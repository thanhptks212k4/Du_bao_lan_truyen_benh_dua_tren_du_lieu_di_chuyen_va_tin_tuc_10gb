{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11911531,"sourceType":"datasetVersion","datasetId":7488421}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/time-series/2015_Zika.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:06:21.211687Z","iopub.execute_input":"2025-06-21T10:06:21.211992Z","iopub.status.idle":"2025-06-21T10:06:23.482318Z","shell.execute_reply.started":"2025-06-21T10:06:21.211964Z","shell.execute_reply":"2025-06-21T10:06:23.481196Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nimport matplotlib.pyplot as plt\nimport os\n\n# ========================\n# 1. ƒê·ªçc & x·ª≠ l√Ω d·ªØ li·ªáu\n# ========================\nfile_path = \"/kaggle/input/time-series/2015_Zika.csv\"\n\ntry:\n    df = pd.read_csv(file_path, on_bad_lines='skip')  # pandas >=1.3.0\nexcept Exception as e:\n    print(f\"L·ªói khi ƒë·ªçc file CSV: {e}\")\n\ndf['date'] = pd.to_datetime(df['date'], errors='coerce')\ndf = df.dropna(subset=['date']).sort_values(by='date').reset_index(drop=True)\ndf = df.drop_duplicates(subset=['username', 'text'])\ndf['OnlyDate'] = df['date'].dt.date\n\ndf_daily = df.groupby('OnlyDate').agg({\n    'retweets': 'sum',\n    'favorites': 'sum',\n    'replies': 'sum',\n    'text': lambda x: x.astype(str).str.len().mean(),\n    'hashtags': lambda x: x.astype(str).str.len().mean()\n}).rename(columns={\n    'retweets': 'retweets_sum',\n    'favorites': 'favorites_sum',\n    'replies': 'replies_sum',\n    'text': 'text_length',\n    'hashtags': 'hashtag_length'\n})\n\narticle_counts = df.groupby('OnlyDate').size()\ndf_daily['num_articles'] = article_counts\n\nfull_index = pd.date_range(start=df_daily.index.min(), end=df_daily.index.max(), freq='D')\ndf_daily = df_daily.reindex(full_index).fillna(0)\ndf_daily.index.name = 'date'\n\n# ========================\n# 2. Chu·∫©n h√≥a & t·∫°o d·ªØ li·ªáu\n# ========================\ndef create_multivariate_dataset(data, window_size=20, target_column=0):\n    X, y = [], []\n    for i in range(len(data) - window_size):\n        X.append(data[i:i + window_size].T)\n        y.append(data[i + window_size, target_column])\n    return np.array(X), np.array(y)\n\nscaler = MinMaxScaler()\ndata_scaled = scaler.fit_transform(df_daily.values)\n\nwindow_size = 20\ntarget_column = df_daily.columns.get_loc('num_articles')\nX_np, y_np = create_multivariate_dataset(data_scaled, window_size, target_column)\n\nX_tensor = torch.tensor(X_np, dtype=torch.float32)\ny_tensor = torch.tensor(y_np, dtype=torch.float32).view(-1, 1)\n\n# ========================\n# 3. M√¥ h√¨nh TCN\n# ========================\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super().__init__()\n        self.chomp_size = chomp_size\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size]\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation, padding, dropout=0.3):\n        super().__init__()\n        self.conv1 = nn.utils.weight_norm(nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation))\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = nn.utils.weight_norm(nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding, dilation=dilation))\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        out = self.dropout1(self.relu1(self.chomp1(self.conv1(x))))\n        out = self.dropout2(self.relu2(self.chomp2(self.conv2(out))))\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.3):\n        super().__init__()\n        layers = []\n        for i in range(len(num_channels)):\n            dilation = 2 ** i\n            in_ch = num_inputs if i == 0 else num_channels[i - 1]\n            out_ch = num_channels[i]\n            padding = (kernel_size - 1) * dilation\n            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, dilation, padding, dropout))\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.network(x)\n\nclass TCNModel(nn.Module):\n    def __init__(self, input_channels):\n        super().__init__()\n        self.tcn = TemporalConvNet(input_channels, [32, 64, 32])\n        self.linear = nn.Linear(32, 1)\n\n    def forward(self, x):\n        y = self.tcn(x)\n        return self.linear(y[:, :, -1])\n\n# ========================\n# 4. Hu·∫•n luy·ªán m√¥ h√¨nh\n# ========================\nmodel = TCNModel(input_channels=X_tensor.shape[1])\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\nbest_loss = float('inf')\npatience = 10\ncounter = 0\nepochs = 300\nloss_history = []\n\nfor epoch in range(epochs):\n    model.train()\n    optimizer.zero_grad()\n    output = model(X_tensor)\n    loss = criterion(output, y_tensor)\n    loss.backward()\n    optimizer.step()\n\n    val_loss = loss.item()\n    loss_history.append(val_loss)\n\n    print(f\"Epoch {epoch+1}: Loss = {val_loss:.6f}\")\n    if val_loss < best_loss - 1e-5:\n        best_loss = val_loss\n        best_model = model.state_dict()\n        counter = 0\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f\"‚õî Early stopping at epoch {epoch+1}\")\n            break\n\nmodel.load_state_dict(best_model)\n\n# ========================\n# 5. ƒê√°nh gi√° m√¥ h√¨nh\n# ========================\nmodel.eval()\nwith torch.no_grad():\n    predictions = model(X_tensor).numpy()\n\nzeros_pad = np.zeros((len(predictions), data_scaled.shape[1]))\nzeros_pad[:, target_column] = predictions.ravel()\ny_pred = scaler.inverse_transform(zeros_pad)[:, target_column]\n\nzeros_pad[:, target_column] = y_np.ravel()\ny_true = scaler.inverse_transform(zeros_pad)[:, target_column]\n\nprint(\"\\nüìä Evaluation Metrics:\")\nprint(f\"R2 Score:  {r2_score(y_true, y_pred):.4f}\")\nprint(f\"MSE:       {mean_squared_error(y_true, y_pred):.4f}\")\nprint(f\"MAE:       {mean_absolute_error(y_true, y_pred):.4f}\")\n\n# ========================\n# 5.5. Ph√¢n lo·∫°i k·∫øt qu·∫£ d·ª± ƒëo√°n th√†nh 3 m·ª©c\n# ========================\nlow_thresh = np.percentile(y_pred, 33)\nhigh_thresh = np.percentile(y_pred, 66)\n\ndef classify_level(value):\n    if value < low_thresh:\n        return \"Th·∫•p\"\n    elif value < high_thresh:\n        return \"Trung b√¨nh\"\n    else:\n        return \"Cao\"\n\npred_levels = np.array([classify_level(val) for val in y_pred])\ndates_pred = df_daily.index[window_size:]\n\nprint(\"\\nüßæ K·∫øt qu·∫£ ph√¢n lo·∫°i d·ª± ƒëo√°n (5 d√≤ng ƒë·∫ßu):\")\nfor i in range(5):\n    print(f\"Ng√†y {dates_pred[i]}: D·ª± ƒëo√°n = {y_pred[i]:.2f} ‚Üí M·ª©c = {pred_levels[i]}\")\n\n# ========================\n# 6. Bi·ªÉu ƒë·ªì k·∫øt qu·∫£\n# ========================\nplt.figure(figsize=(12, 5))\nplt.plot(y_true, label='Th·ª±c t·∫ø', linewidth=2)\nplt.plot(y_pred, label='D·ª± ƒëo√°n', linestyle='--')\nplt.title(\"D·ª± ƒëo√°n s·ªë b√†i b√°o m·ªói ng√†y b·∫±ng TCN\")\nplt.xlabel(\"Ng√†y\")\nplt.ylabel(\"S·ªë b√†i b√°o\")\nplt.grid(True)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# ========================\n# 7. Bi·ªÉu ƒë·ªì Loss theo Epoch\n# ========================\nplt.figure(figsize=(10, 4))\nplt.plot(loss_history, color='blue', linewidth=2)\nplt.title(\"Bi·ªÉu ƒë·ªì Loss theo Epoch\", fontsize=14)\nplt.xlabel(\"Epoch\", fontsize=12)\nplt.ylabel(\"Loss (MSE)\", fontsize=12)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T10:49:10.002598Z","iopub.execute_input":"2025-06-21T10:49:10.003674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#L∆∞u m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán (Zika)\n# ========================\nmodel_path = \"tcn_zika_model.pth\"\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'input_channels': X_tensor.shape[1]\n}, model_path)\n\nprint(f\"\\n‚úÖ M√¥ h√¨nh TCN (Zika) ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}